{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot -> Remember previous interactions\n",
    "\n",
    "-> Using GROQ api to call the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() \n",
    "\n",
    "# langsmith tracking\n",
    "#https://smith.langchain.com/\n",
    "# os.environ['LANGCHAIN_API_KEY']=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "# os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "#groqAPIkey = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model='Gemma2-9b-It')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Subodh!\\n\\nIt's great to meet you.  \\n\\nWhat kind of data science work do you do? Are you working on any interesting projects right now? \\n\\nI'm always eager to learn more about how people are using data science to solve problems and make a difference.\\n\", response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 22, 'total_tokens': 86, 'completion_time': 0.116363636, 'prompt_time': 0.000191129, 'queue_time': 0.014145101, 'total_time': 0.116554765}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ce35d1c0-f02b-443b-9e41-c4e79ce80c38-0', usage_metadata={'input_tokens': 22, 'output_tokens': 64, 'total_tokens': 86})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke([HumanMessage(content=\"Hi, I am Subodh, working as a data scientist\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a large language model, I have no memory of past conversations. If you'd like to tell me your name, I'd be happy to know! ðŸ˜Š  \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 23, 'total_tokens': 62, 'completion_time': 0.079614143, 'prompt_time': 0.002610718, 'queue_time': None, 'total_time': 0.082224861}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-4e5a8f45-1cdc-4994-83b3-c3de8cd0c721-0', usage_metadata={'input_tokens': 23, 'output_tokens': 39, 'total_tokens': 62})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is my name? i told you \") # here conversation history does not know to LLM\n",
    "\n",
    "# Model does not have no memory of past conversations. \n",
    "# Every interaction we have is a fresh start.\\n\\nIf you'd like to tell me your name, I'm happy to remember it for this conversation! ðŸ˜„  \\n\\n\", response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 23, 'total_tokens': 75, 'completion_time': 0.104774858, 'prompt_time': 0.002321479, 'queue_time': None, 'total_time': 0.107096337}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3837de9a-3d76-4765-a824-8bb03177d3b2-0', usage_metadata={'input_tokens': 23, 'output_tokens': 52, 'total_tokens': 75})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIMessage\n\u001b[1;32m----> 2\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241m.\u001b[39minvoke([\n\u001b[0;32m      3\u001b[0m         HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, I am Subodh, working as a data scientist\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      4\u001b[0m         AIMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi Subodh!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms great to meet you.  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWhat kind of data science work do you do?  I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm always interested in learning more about how people are using AI and data analysis in different fields.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m        , HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHey, What is my name and what do i do?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ]\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "llm.invoke([\n",
    "        HumanMessage(content=\"Hi, I am Subodh, working as a data scientist\"),\n",
    "        AIMessage(content=\"Hi Subodh!\\n\\nIt's great to meet you.  \\n\\nWhat kind of data science work do you do?  I'm always interested in learning more about how people are using AI and data analysis in different fields.\")\n",
    "       , HumanMessage(content=\"Hey, What is my name and what do i do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History\n",
    "-> session: for particular user we maintain session. \n",
    "\n",
    "In current interaction with LLM, it will load the previos interactions and\n",
    "\n",
    "then send the current input + history to LLM\n",
    "\n",
    "\n",
    "RunnableWithMessageHistory: A class that runs an LLM (Language Model) while maintaining message history, allowing for continuous conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# define LLM\n",
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model='Gemma2-9b-It')\n",
    "\n",
    "# this fuction take session id as input, it gives the history of that chat and returns it\n",
    "# means = for specifice session id what is the chat history present. \n",
    "store = {}\n",
    "def getSessionhistory(sessionId:str)->BaseChatMessageHistory: # return type for manage structure of history\n",
    "    if sessionId not in store:\n",
    "        store[sessionId] = ChatMessageHistory()\n",
    "    return store[sessionId]\n",
    "\n",
    "\n",
    "with_messageHistory = RunnableWithMessageHistory(llm, getSessionhistory)\n",
    "\n",
    "config = {\"configurable\":{\"session_id\":\"chat1\"}} # it contain at what session id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =with_messageHistory.invoke(\n",
    "    [HumanMessage(content=\"hi, i am Subodh, I am working in wipro as a data scientist\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Subodh,\\n\\nThat's great to hear! It's fantastic that you're working as a Data Scientist at Wipro.  \\n\\nData science is a rapidly growing field with lots of exciting opportunities. \\n\\nWhat kind of projects are you working on at the moment? Are there any particular areas of data science that you're particularly interested in? \\n\\nI'd love to hear more about your work!\\n\", response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 27, 'total_tokens': 118, 'completion_time': 0.165454545, 'prompt_time': 0.000156859, 'queue_time': 0.012960900999999999, 'total_time': 0.165611404}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-780ed92f-5df1-4579-9da0-afab37815d26-0', usage_metadata={'input_tokens': 27, 'output_tokens': 91, 'total_tokens': 118})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='hi, i am Subodh, I am working in wipro as a data scientist'), AIMessage(content=\"Hello Subodh,\\n\\nThat's great to hear! It's fantastic that you're working as a Data Scientist at Wipro.  \\n\\nData science is a rapidly growing field with lots of exciting opportunities. \\n\\nWhat kind of projects are you working on at the moment? Are there any particular areas of data science that you're particularly interested in? \\n\\nI'd love to hear more about your work!\\n\", response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 27, 'total_tokens': 118, 'completion_time': 0.165454545, 'prompt_time': 0.000156859, 'queue_time': 0.012960900999999999, 'total_time': 0.165611404}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-780ed92f-5df1-4579-9da0-afab37815d26-0', usage_metadata={'input_tokens': 27, 'output_tokens': 91, 'total_tokens': 118})])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Subodh. \\n\\nYou told me at the beginning of our conversation!  ðŸ˜Š  \\n\\nIs there anything else I can help you with?\\n' response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 130, 'total_tokens': 166, 'completion_time': 0.065454545, 'prompt_time': 0.003587151, 'queue_time': 0.009835809, 'total_time': 0.069041696}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-055a9082-687d-49a2-818d-bf71735bec1f-0' usage_metadata={'input_tokens': 130, 'output_tokens': 36, 'total_tokens': 166}\n"
     ]
    }
   ],
   "source": [
    "def ask_Que(user):\n",
    "    response=with_messageHistory.invoke([\n",
    "                HumanMessage(content=user)\n",
    "            ] , config=config)\n",
    "    print(response) \n",
    "ask_Que(\"What is my name\")\n",
    "\n",
    "# invoke: This method sends a message to the LLM and processes it within the context of the session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='hi, i am Subodh, I am working in wipro as a data scientist'), AIMessage(content=\"Hello Subodh,\\n\\nThat's great to hear! It's fantastic that you're working as a Data Scientist at Wipro.  \\n\\nData science is a rapidly growing field with lots of exciting opportunities. \\n\\nWhat kind of projects are you working on at the moment? Are there any particular areas of data science that you're particularly interested in? \\n\\nI'd love to hear more about your work!\\n\", response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 27, 'total_tokens': 118, 'completion_time': 0.165454545, 'prompt_time': 0.000156859, 'queue_time': 0.012960900999999999, 'total_time': 0.165611404}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-780ed92f-5df1-4579-9da0-afab37815d26-0', usage_metadata={'input_tokens': 27, 'output_tokens': 91, 'total_tokens': 118}), HumanMessage(content='What is my name'), AIMessage(content='Your name is Subodh. \\n\\nYou told me at the beginning of our conversation!  ðŸ˜Š  \\n\\nIs there anything else I can help you with?\\n', response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 130, 'total_tokens': 166, 'completion_time': 0.065454545, 'prompt_time': 0.003587151, 'queue_time': 0.009835809, 'total_time': 0.069041696}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-055a9082-687d-49a2-818d-bf71735bec1f-0', usage_metadata={'input_tokens': 130, 'output_tokens': 36, 'total_tokens': 166})])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"You are chatting with me, an AI assistant! \\n\\nYou told me you are Subodh and that you work as a Data Scientist at Wipro. \\n\\nIs there anything specific you'd like to talk about or ask me? I'm here to help! ðŸ˜Š \\n\" response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 179, 'total_tokens': 240, 'completion_time': 0.110909091, 'prompt_time': 0.006902584, 'queue_time': 0.0069755749999999995, 'total_time': 0.117811675}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-8e91d15f-943e-4f29-a044-fdff3d79e44e-0' usage_metadata={'input_tokens': 179, 'output_tokens': 61, 'total_tokens': 240}\n"
     ]
    }
   ],
   "source": [
    "def ask_Que(user):\n",
    "    response=with_messageHistory.invoke([\n",
    "                HumanMessage(content=user)\n",
    "            ] , config=config)\n",
    "    print(response)\n",
    "ask_Que(\"What i am doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='hi, i am Subodh, I am working in wipro as a data scientist'), AIMessage(content=\"Hello Subodh,\\n\\nThat's great to hear! It's fantastic that you're working as a Data Scientist at Wipro.  \\n\\nData science is a rapidly growing field with lots of exciting opportunities. \\n\\nWhat kind of projects are you working on at the moment? Are there any particular areas of data science that you're particularly interested in? \\n\\nI'd love to hear more about your work!\\n\", response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 27, 'total_tokens': 118, 'completion_time': 0.165454545, 'prompt_time': 0.000156859, 'queue_time': 0.012960900999999999, 'total_time': 0.165611404}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-780ed92f-5df1-4579-9da0-afab37815d26-0', usage_metadata={'input_tokens': 27, 'output_tokens': 91, 'total_tokens': 118}), HumanMessage(content='What is my name'), AIMessage(content='Your name is Subodh. \\n\\nYou told me at the beginning of our conversation!  ðŸ˜Š  \\n\\nIs there anything else I can help you with?\\n', response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 130, 'total_tokens': 166, 'completion_time': 0.065454545, 'prompt_time': 0.003587151, 'queue_time': 0.009835809, 'total_time': 0.069041696}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-055a9082-687d-49a2-818d-bf71735bec1f-0', usage_metadata={'input_tokens': 130, 'output_tokens': 36, 'total_tokens': 166}), HumanMessage(content='What i am doing?'), AIMessage(content=\"You are chatting with me, an AI assistant! \\n\\nYou told me you are Subodh and that you work as a Data Scientist at Wipro. \\n\\nIs there anything specific you'd like to talk about or ask me? I'm here to help! ðŸ˜Š \\n\", response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 179, 'total_tokens': 240, 'completion_time': 0.110909091, 'prompt_time': 0.006902584, 'queue_time': 0.0069755749999999995, 'total_time': 0.117811675}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-8e91d15f-943e-4f29-a044-fdff3d79e44e-0', usage_metadata={'input_tokens': 179, 'output_tokens': 61, 'total_tokens': 240})])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage,SystemMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "(\"system\",\"You are a helpful assistant. Answer all the question to the best of your ability in {language} language.  \"),\n",
    "MessagesPlaceholder(variable_name=\"messages\")\n",
    "\n",
    "])\n",
    "\n",
    "chain = prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='à¤¨à¤®à¤¸à¥à¤¤à¥‡ Subodh,  à¤®à¥ˆà¤‚ à¤†à¤ªà¤•à¥€ à¤®à¤¦à¤¦ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¯à¤¹à¤¾à¤ à¤¹à¥‚à¤à¥¤ \\n\\nà¤†à¤ª à¤®à¥à¤à¤¸à¥‡ à¤•à¥‹à¤ˆ à¤­à¥€ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤ªà¥‚à¤› à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤”à¤° à¤®à¥ˆà¤‚ à¤…à¤ªà¤¨à¥€ à¤ªà¥‚à¤°à¥€ à¤•à¥à¤·à¤®à¤¤à¤¾ à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤° à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤¨à¥‡ à¤•à¥€ à¤•à¥‹à¤¶à¤¿à¤¶ à¤•à¤°à¥‚à¤à¤—à¤¾à¥¤ \\n\\n', response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 37, 'total_tokens': 96, 'completion_time': 0.107272727, 'prompt_time': 0.000395548, 'queue_time': 0.012831351999999999, 'total_time': 0.107668275}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f5026073-a2e5-4d53-b366-520aeab88cdc-0', usage_metadata={'input_tokens': 37, 'output_tokens': 59, 'total_tokens': 96})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Subodh\")] , \n",
    "              \"language\":\"Hindi\"})\n",
    "#chain.invoke({\"messages\":[\"Hi my name is Subodh\" ], \n",
    "#              \"language\":\"Hindi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "withMessageHistory = RunnableWithMessageHistory(chain, getSessionhistory, input_messages_key=\"messages\") # chain is send as a input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\":{\"session_id\":\"chat2\"}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¨à¤®à¤¸à¥à¤¤à¥‡ Subodh! ðŸ˜Š  \\n\\nà¤®à¥à¤à¥‡ à¤†à¤ªà¤¸à¥‡ à¤®à¤¿à¤²à¤•à¤° à¤–à¥à¤¶à¥€ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤  \\nà¤†à¤ª à¤®à¥à¤à¤¸à¥‡ à¤•à¥à¤¯à¤¾ à¤ªà¥‚à¤›à¤¨à¤¾ à¤šà¤¾à¤¹à¥‡à¤‚à¤—à¥‡? \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = withMessageHistory.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi my name is Subodh\")], \n",
    "     \"language\": \"hindi\"}, \n",
    "    config=config\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¨à¤®à¤¸à¥à¤¤à¥‡ Subodh! ðŸ˜Š  \\n\\nà¤®à¥à¤à¥‡ à¤†à¤ªà¤¸à¥‡ à¤®à¤¿à¤²à¤•à¤° à¤–à¥à¤¶à¥€ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤  \\nà¤†à¤ª à¤®à¥à¤à¤¸à¥‡ à¤•à¥à¤¯à¤¾ à¤ªà¥‚à¤›à¤¨à¤¾ à¤šà¤¾à¤¹à¥‡à¤‚à¤—à¥‡? \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤†à¤ªà¤•à¤¾ à¤¨à¤¾à¤® Subodh à¤¹à¥ˆà¥¤  \\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = withMessageHistory.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name\")], \n",
    "     \"language\": \"hindi\"}, \n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes history becomes very long \n",
    "\n",
    "Manage the conversation history\n",
    "\n",
    "trip messages => helps to reduce how many messages we are sending to model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 27\u001b[0m\n\u001b[0;32m      3\u001b[0m trimmer \u001b[38;5;241m=\u001b[39m trim_messages(\n\u001b[0;32m      4\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m35\u001b[39m,\n\u001b[0;32m      5\u001b[0m     strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     start_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     SystemMessage(content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are the good AI assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[0;32m     14\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     AIMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m ]\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrimmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4475\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4461\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[0;32m   4462\u001b[0m \n\u001b[0;32m   4463\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4472\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[0;32m   4473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4476\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4477\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4478\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4479\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4480\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   4483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4485\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1785\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1782\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1783\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1784\u001b[0m         Output,\n\u001b[1;32m-> 1785\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1795\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:397\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    396\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4331\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   4329\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   4330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4331\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4332\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   4333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4334\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[0;32m   4335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:397\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    396\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:803\u001b[0m, in \u001b[0;36mtrim_messages\u001b[1;34m(messages, max_tokens, token_counter, strategy, allow_partial, end_on, start_on, include_system, text_splitter)\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _first_max_tokens(\n\u001b[0;32m    795\u001b[0m         messages,\n\u001b[0;32m    796\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    800\u001b[0m         end_on\u001b[38;5;241m=\u001b[39mend_on,\n\u001b[0;32m    801\u001b[0m     )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_last_max_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_token_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_partial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_system\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_system\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_splitter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m. Supported strategies are \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:918\u001b[0m, in \u001b[0;36m_last_max_tokens\u001b[1;34m(messages, max_tokens, token_counter, text_splitter, allow_partial, include_system, start_on, end_on)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    916\u001b[0m     reversed_ \u001b[38;5;241m=\u001b[39m messages[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 918\u001b[0m reversed_ \u001b[38;5;241m=\u001b[39m \u001b[43m_first_max_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreversed_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_splitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped_system:\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reversed_[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m reversed_[\u001b[38;5;241m1\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:833\u001b[0m, in \u001b[0;36m_first_max_tokens\u001b[1;34m(messages, max_tokens, token_counter, text_splitter, partial_strategy, end_on)\u001b[0m\n\u001b[0;32m    831\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(messages)):\n\u001b[1;32m--> 833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken_counter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_tokens:\n\u001b[0;32m    834\u001b[0m         idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m-\u001b[39m i\n\u001b[0;32m    835\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:377\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_num_tokens_from_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_num_tokens_from_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: List[BaseMessage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the number of tokens in the messages.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m    Useful for checking if an input fits in a model's context window.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m        The sum of the number of tokens across the messages.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_buffer_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:377\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_num_tokens_from_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: List[BaseMessage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the number of tokens in the messages.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m    Useful for checking if an input fits in a model's context window.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m        The sum of the number of tokens across the messages.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_buffer_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages])\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:364\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_num_tokens\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the number of tokens present in the text.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m    Useful for checking if an input fits in a model's context window.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m        The integer number of tokens in the text.\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:351\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_token_ids\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_get_token_ids(text)\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_token_ids_default_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:80\u001b[0m, in \u001b[0;36m_get_token_ids_default_method\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode the text into token IDs.\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# get the cached tokenizer\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:66\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a GPT-2 tokenizer instance.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03mThis function is cached to avoid re-loading the tokenizer\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03mevery time it is called.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2TokenizerFast  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\transformers\\utils\\__init__.py:34\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     28\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     ContextManagers,\n\u001b[0;32m     36\u001b[0m     ExplicitEnum,\n\u001b[0;32m     37\u001b[0m     ModelOutput,\n\u001b[0;32m     38\u001b[0m     PaddingStrategy,\n\u001b[0;32m     39\u001b[0m     TensorType,\n\u001b[0;32m     40\u001b[0m     add_model_info_to_auto_map,\n\u001b[0;32m     41\u001b[0m     add_model_info_to_custom_pipelines,\n\u001b[0;32m     42\u001b[0m     cached_property,\n\u001b[0;32m     43\u001b[0m     can_return_loss,\n\u001b[0;32m     44\u001b[0m     expand_dims,\n\u001b[0;32m     45\u001b[0m     filter_out_non_signature_kwargs,\n\u001b[0;32m     46\u001b[0m     find_labels,\n\u001b[0;32m     47\u001b[0m     flatten_dict,\n\u001b[0;32m     48\u001b[0m     infer_framework,\n\u001b[0;32m     49\u001b[0m     is_jax_tensor,\n\u001b[0;32m     50\u001b[0m     is_numpy_array,\n\u001b[0;32m     51\u001b[0m     is_tensor,\n\u001b[0;32m     52\u001b[0m     is_tf_symbolic_tensor,\n\u001b[0;32m     53\u001b[0m     is_tf_tensor,\n\u001b[0;32m     54\u001b[0m     is_torch_device,\n\u001b[0;32m     55\u001b[0m     is_torch_dtype,\n\u001b[0;32m     56\u001b[0m     is_torch_tensor,\n\u001b[0;32m     57\u001b[0m     reshape,\n\u001b[0;32m     58\u001b[0m     squeeze,\n\u001b[0;32m     59\u001b[0m     strtobool,\n\u001b[0;32m     60\u001b[0m     tensor_size,\n\u001b[0;32m     61\u001b[0m     to_numpy,\n\u001b[0;32m     62\u001b[0m     to_py_obj,\n\u001b[0;32m     63\u001b[0m     torch_float,\n\u001b[0;32m     64\u001b[0m     torch_int,\n\u001b[0;32m     65\u001b[0m     transpose,\n\u001b[0;32m     66\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     70\u001b[0m     HF_MODULES_CACHE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     try_to_load_from_cache,\n\u001b[0;32m     97\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     99\u001b[0m     ACCELERATE_MIN_VERSION,\n\u001b[0;32m    100\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     torch_only_method,\n\u001b[0;32m    220\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:462\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 462\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_torch_pytree\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_model_output_flatten\u001b[39m(output: ModelOutput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_pytree.Context\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;28mlist\u001b[39m(output\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\Admin\\Desktop\\GenAI Krishna naik\\Langchain Basics\\.venv\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import trim_messages, AIMessage,HumanMessage, SystemMessage\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 35,\n",
    "    strategy = \"last\",\n",
    "    token_counter = llm , # genai model\n",
    "    include_system = True,\n",
    "    allow_partial = False,\n",
    "    start_on = 'human'\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content = \"You are the good AI assistant.\"), \n",
    "\n",
    "    HumanMessage(content = \"Hi, I am a bod\"),\n",
    "    AIMessage(content=\"Hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice-cream\"),\n",
    "    AIMessage(content=\"Nice\"),\n",
    "    HumanMessage(content=\"What is 2+2\"),\n",
    "    AIMessage(content=\"ans is 4\"),\n",
    "    HumanMessage(content=\"Thanks\"),\n",
    "    AIMessage(content=\"No Problem\"),\n",
    "    HumanMessage(content=\"Having Fun\"),\n",
    "    AIMessage(content=\"Yes\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.11 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 16.6 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 widgetsnbextension-4.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\desktop\\genai krishna naik\\langchain basics\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
