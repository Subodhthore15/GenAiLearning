{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain expression language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# langsmith tracking\n",
    "#https://smith.langchain.com/\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "#groqAPIkey = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "LLM=ChatGroq(model='gemma2-9b-it', temperature= 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI engineer, I can tell you that **Generative AI** is a fascinating and rapidly evolving field within artificial intelligence. \\n\\nHere's a breakdown:\\n\\n**What it is:**\\n\\nGenerative AI refers to a type of artificial intelligence that focuses on creating new content. This content can take many forms, including:\\n\\n* **Text:**  Articles, stories, poems, dialogue, code\\n* **Images:** Photos, artwork, illustrations\\n* **Audio:** Music, sound effects, speech\\n* **Video:** Animations, short films\\n* **Other data:** 3D models, synthetic datasets\\n\\n**How it works:**\\n\\nGenerative AI models are typically trained on massive datasets of existing content. They learn the underlying patterns and structures within this data, allowing them to generate new content that resembles the training data. \\n\\nCommon techniques used in generative AI include:\\n\\n* **Generative Adversarial Networks (GANs):**  Two neural networks compete against each other. One network generates content, while the other tries to distinguish real content from generated content. This adversarial process pushes both networks to improve, resulting in increasingly realistic generated output.\\n* **Transformer Networks:**  These networks are particularly good at understanding and generating text. They use attention mechanisms to focus on relevant parts of the input data, allowing them to capture long-range dependencies and generate coherent text.\\n\\n**Examples:**\\n\\n* **ChatGPT:** A powerful language model that can generate human-like text in response to prompts.\\n* **DALL-E 2:**  An AI system that can create realistic images from text descriptions.\\n* **Jukebox:**  An AI model that can generate music in various styles.\\n\\n**Applications:**\\n\\nGenerative AI has a wide range of potential applications, including:\\n\\n* **Creative industries:**  Generating art, music, and writing.\\n* **Marketing and advertising:**  Creating personalized content and ad campaigns.\\n* **Education:**  Generating interactive learning materials.\\n* **Healthcare:**  Generating synthetic medical images for training and research.\\n* **Research:**  Creating new datasets for scientific exploration.\\n\\n**Ethical considerations:**\\n\\nThe power of generative AI also raises ethical concerns, such as:\\n\\n* **Misinformation and deepfakes:**  The ability to generate realistic fake content can be used for malicious purposes.\\n* **Bias and fairness:**  AI models can inherit and amplify biases present in the training data.\\n* **Copyright and intellectual property:**  Questions arise about the ownership and copyright of AI-generated content.\\n\\n\\nIt's a field with immense potential, but it's crucial to develop and use generative AI responsibly.\\n\", response_metadata={'token_usage': {'completion_tokens': 547, 'prompt_tokens': 27, 'total_tokens': 574, 'completion_time': 1.139792567, 'prompt_time': 0.002610928, 'queue_time': None, 'total_time': 1.142403495}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-b50f156f-cbfc-4819-8c00-fad976e2a26e-0', usage_metadata={'input_tokens': 27, 'output_tokens': 547, 'total_tokens': 574})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI engineer expert.\"),\n",
    "    (\"user\", \"Question: {que}\") \n",
    "])\n",
    "chain = prompt|LLM\n",
    "chain.invoke({'que':\"what is generativeAI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour, comment allez-vous ? \\n\\n\\nYou can also use the more informal:\\n\\n* Salut, ça va ? \\n', response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 27, 'total_tokens': 55, 'completion_time': 0.055472449, 'prompt_time': 0.002578508, 'queue_time': None, 'total_time': 0.058050957}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-a9e88f3b-3732-41de-837c-5b54cb02e728-0', usage_metadata={'input_tokens': 27, 'output_tokens': 28, 'total_tokens': 55})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from english to french\"),\n",
    "    HumanMessage(content=\"hello, how are you?\")\n",
    "]\n",
    "\n",
    "LLM.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, comment allez-vous ? \\n\\n\\nYou can also use the more informal:\\n\\n* Salut, ça va ? \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL ----> chain the component together\n",
    "\n",
    "chain = LLM|parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "genericTemplate= \"Convert  the following into {DBName} format.\"\n",
    "\n",
    "few_shot_dict = [\n",
    "    {\"question\": \"how to calculate total employees from table A\",\n",
    "     \"answer\": \"The SQL query: 'SELECT COUNT(DISTINCT *) FROM table_A;'\"},\n",
    "\n",
    "    {\"question\": \"how to find average salary from table B\",\n",
    "     \"answer\": \"The SQL query: 'SELECT AVG(salary) FROM table_B;'\"} ,\n",
    "\n",
    "     {\"question\": \"Hi or hello\",\n",
    "     \"answer\": \"How can i help you sirg? \"}\n",
    "]\n",
    "\n",
    "# Convert dictionary to list of HumanMessage and AIMessage\n",
    "few_shot_examples = []\n",
    "for example in few_shot_dict:\n",
    "    few_shot_examples.append(HumanMessage(content=example['question']))\n",
    "    few_shot_examples.append(AIMessage(content=example['answer']))\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"\"\"You are the natural language to SQL convertor translate the. {genericTemplate} so be mindful.\n",
    "          I have provided you some examples also.\n",
    "          If user question is not related to SQL then don't show any random response just say 'Don't know sir' \"\"\"),\n",
    "        *few_shot_examples,\n",
    "        (\"user\",\"User question is: {text}. Give the sql query int the provided format.\")\n",
    "    ]\n",
    ") \n",
    "\n",
    "\n",
    "# prompt.invoke({\"DBName\":\"Mysql workbench\",\"text\":\"how are you bro\"})\n",
    "# chain = prompt|LLM |parser\n",
    "# chain.invoke({\"DBName\":\"Mysql workbench\",\"text\":\"how many department are there?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are the natural language to SQL convertor translate the.\"+genericTemplate +\" so be mindful.\\n          I have provided you some examples also.\\n          If user question is not related to SQL then don\\'t show any random response just say \\'Don\\'t know sir\\' '), HumanMessage(content='how to calculate total employees from table A'), AIMessage(content=\"The SQL query: 'SELECT COUNT(DISTINCT *) FROM table_A;'\"), HumanMessage(content='how to find average salary from table B'), AIMessage(content=\"The SQL query: 'SELECT AVG(salary) FROM table_B;'\"), HumanMessage(content='Hi or hello'), AIMessage(content='How can i help you sirg? '), HumanMessage(content='User question is: how are you bro. Give the sql query int the provided format.')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"DBName\":\"Mysql workbench\",\"text\":\"how are you bro\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT(DISTINCT department_name) FROM table_name; \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt|LLM |parser\n",
    "chain.invoke({\"DBName\":\"Mysql workbench\",\"text\":\"how many department are there?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working but  with message Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate(\n",
    "    \n",
    "    [\n",
    "        (\"system\",\"You are the AI assistant, answer the question in {language} language. \"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\",\"user question is {input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|llm\n",
    "chain.invoke({\"messages\":few_shot_examples, \"language\":\"Marathi\",\"input\":\"exaplain subodh details\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "few_shot_dict = [\n",
    "    {\"question\": \"What is subodh height?\",\n",
    "     \"answer\": \"Subodh Height is 6 feet 4 inches\"},\n",
    "\n",
    "    {\"question\": \"Where subodh is working?\",\n",
    "     \"answer\": \"Subodh is working in Wipro company\"} ,\n",
    "\n",
    "     {\"question\":\"What technologies subodh working on?\",\n",
    "      \"answer\":\"he is working on Data science\"},\n",
    "\n",
    "     ]\n",
    "\n",
    "# Convert dictionary to list of HumanMessage and AIMessage\n",
    "few_shot_examples = []\n",
    "for example in few_shot_dict:\n",
    "    few_shot_examples.append(HumanMessage(content=example['question']))\n",
    "    few_shot_examples.append(AIMessage(content=example['answer']))\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"\"\"You are the AI agent. \n",
    "         \"\"\"),\n",
    "        *few_shot_examples,\n",
    "        (\"user\",\"User has asked you about: {text}. \")\n",
    "    ]\n",
    ") \n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "llm.invoke([\n",
    "        HumanMessage(content=\"Hi, I am Subodh, working as a data scientist\"),\n",
    "        AIMessage(content=\"Hi Subodh!\\n\\nIt's great to meet you.  \\n\\nWhat kind of data science work do you do?  I'm always interested in learning more about how people are using AI and data analysis in different fields.\")\n",
    "       , HumanMessage(content=\"Hey, What is my name and what do i do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
