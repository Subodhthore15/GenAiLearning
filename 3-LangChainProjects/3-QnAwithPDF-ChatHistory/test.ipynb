{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_groq import ChatGroq  \n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import os \n",
    "\n",
    "# doubts in [create_history_aware_retriever, MessagesPlaceholder, RunnableWithMessageHistory]\n",
    "# how chat history stored\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## LLM model\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "## Embeddings: 2304\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2:2b\")\n",
    "\n",
    "loader = PyPDFLoader(r\"temp.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents and create embeddings\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size = 1500, chunk_overlap = 200)\n",
    "splittedDocs=textSplitter.split_documents(docs)\n",
    "\n",
    "vectorStore = FAISS.from_documents(documents=splittedDocs, embedding= embeddings)\n",
    "retrieverDB  = vectorStore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=vectorStore._embed_query(\"hwllo how are you\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "        \"\"\" \n",
    "            Given a chat history and the latest user question. \n",
    "            which might reference context in the chat history \n",
    "            Formulate a standalone question which can be understood. \n",
    "            without the chat history, Do not answer the question. Just formulate it if needed\n",
    "            otherwise return it  as it is. \n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\",\"{input}\")\n",
    "        ]\n",
    "    )\n",
    "# it's task is to take the user input, then rephrase it by llm using history,\n",
    "# Then rephrase question is passed to vectorRetriver for getting documents context\n",
    "\n",
    "history_awareRetriever=create_history_aware_retriever(llm, retrieverDB, contextualize_q_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B07B5A0350>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=' \\n            Given a chat history and the latest user question. \\n            which might reference context in the chat history \\n            Formulate a standalone question which can be understood. \\n            without the chat history. Do not answer the question. Just formulate it if needed\\n            otherwise return it  as it is. \\n        ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B07FCBA8D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B07FCB9590>, model_name='gemma2-9b-it', groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B07B5A0350>)), config={'run_name': 'chat_retriever_chain'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_awareRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "        \"\"\" \n",
    "            You are the assistant for question answering tasks. \n",
    "            Use the following pieces of retriever context to answer the question.\n",
    "            If you don't know the answer, Say thank you don't know. Use the three sentences maximum\n",
    "            and keep the answer concise.\n",
    "            \\n\\n\n",
    "            {context}.\n",
    "        \"\"\"\n",
    "    )\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\",\"{input}\")\n",
    "        ]\n",
    "    )\n",
    "# context fetch by histrory aware retriever is passed to create_stuff_documents_chain\n",
    "# and it is responsible for giving \"context\" to system prompt or LLM. & generate the response. \n",
    "\n",
    "que_ans_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_awareRetriever, que_ans_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\" \\n            You are the assistant for question answering tasks. \\n            Use the following pieces of retriever context to answer the question.\\n            If you don't know the answer, Say thank you don't know. Use the three sentences maximum\\n            and keep the answer concise.\\n            \\n\\n\\n            {context}.\\n        \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B07FCBA8D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B07FCB9590>, model_name='gemma2-9b-it', groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_ans_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B07B5A0350>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=' \\n            Given a chat history and the latest user question. \\n            which might reference context in the chat history \\n            Formulate a standalone question which can be understood. \\n            without the chat history. Do not answer the question. Just formulate it if needed\\n            otherwise return it  as it is. \\n        ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "           | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B07FCBA8D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B07FCB9590>, model_name='gemma2-9b-it', groq_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B07B5A0350>)), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\" \\n            You are the assistant for question answering tasks. \\n            Use the following pieces of retriever context to answer the question.\\n            If you don't know the answer, Say thank you don't know. Use the three sentences maximum\\n            and keep the answer concise.\\n            \\n\\n\\n            {context}.\\n        \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B07FCBA8D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B07FCB9590>, model_name='gemma2-9b-it', groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history for session\n",
    "\n",
    "store= {}\n",
    "def get_session_history(session_id:str)-> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "            rag_chain, get_session_history, input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "            output_messages_key=\"answer\" # optional\n",
    "    )   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "user_input = \"what is saving\"\n",
    "\n",
    "response = conversational_rag_chain.invoke(\n",
    "            {\"input\":user_input},\n",
    "            config = {\n",
    "                \"configurable\":{\"session_id\":\"subodh1\"}\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saving involves setting aside money for future use, typically focusing on short-term needs and preserving capital.  \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"what question i asked you before\"\n",
    "\n",
    "response = conversational_rag_chain.invoke(\n",
    "            {\"input\":user_input},\n",
    "            config = {\n",
    "                \"configurable\":{\"session_id\":\"subodh1\"}\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked: \"what is saving\" \\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
